{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ed4aee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; vertical-align: middle;\">Numerical Methods in Accelerator Physics</h1>\n",
    "<h2 style=\"text-align: center; vertical-align: middle;\">Lecture Series by Dr. Adrian Oeftiger</h2>\n",
    "\n",
    "<h3 style=\"text-align: center; vertical-align: middle; margin-top: 1em; margin-bottom: 1em;\">Guest Lecture by Dr. Michael Schenk</h3>\n",
    "\n",
    "<div style=\"width: 65%; margin: auto; margin-top: 1em; \">\n",
    "<img src=\"./img/etit.png\" style=\"width: 20%; float: left; margin-right: 5%;\" /><img src=\"./img/GSI_Logo.png\" style=\"width: 25%; float: left; margin-right: 5%; padding-top: 0.5em;\" /><img src=\"./img/FAIR_Logo.png\" style=\"width: 15%; float: left; margin-right: 5%;\" /><img src=\"./img/CERN_Logo.svg\" style=\"width: 12%; float: left; padding: 1%;\" />\n",
    "</div>\n",
    "\n",
    "<h3 style=\"clear: both; text-align: center; margin-top: 1em;\">Lecture 12</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d1b16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Run this notebook online!</h2>\n",
    "\n",
    "Interact and run this jupyter notebook online:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"text-align:center;\">\n",
    "via the public mybinder.org service: <br />\n",
    "<a href=\"https://mybinder.org/v2/gh/aoeftiger/TUDa-NMAP-2023/v12.0\" style=\"width:auto; display:table;margin:0.5em auto;\"><img src=\"./img/binder_logo.svg\" alt=\"mybinder.org logo\" height=\"1ex\" /></a>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"text-align:center;\">\n",
    "via the public gesis.org service: <br />\n",
    "<a href=\"https://notebooks.gesis.org/binder/v2/gh/aoeftiger/TUDA-NMAP-2023/v12.0\" style=\"width:auto; display:table;margin:0.5em auto;\"><img src=\"./img/binder_logo.svg\" alt=\"gesis.org logo\" height=\"1ex\" /></a>\n",
    "</div>\n",
    "\n",
    "Also find this lecture rendered [as HTML slides on github $\\nearrow$](https://aoeftiger.github.io/TUDa-NMAP-2023/) along with the [source repository $\\nearrow$](https://github.com/aoeftiger/TUDa-NMAP-2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36bcfd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Run this first!</h2>\n",
    "\n",
    "Imports and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cf36a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from config import (print_qtable, Maze, QLearner, plot_q_table, \n",
    "                    plot_greedy_policy, tf, e_trajectory,\n",
    "                    ClassicalDDPG, trainer, plot_training_log, run_correction)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da38d4",
   "metadata": {},
   "source": [
    "If the progress bar by `tqdm` (`trange`) in this document does not work, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d33a62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Refresher!</h2>\n",
    "\n",
    "- Bayesian Optimisation (BO) as _global_ optimisation method for black-box functions\n",
    "- _Gaussian Process_ (GP): statistical surrogate model\n",
    "- _Acquisition Function_: guide the optimisation\n",
    "- BO suitable for __optimisation__ &ndash; not for _control_ tasks\n",
    "- adequate for moderate amount of dimensions (~100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a1851",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Today!</h2>\n",
    "\n",
    "1. Introduction to Reinforcement Learning\n",
    "2. Reinforcement Learning Formalism\n",
    "3. Q-learning\n",
    "4. Actor-critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304eea41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h4>Disclaimer</h4>\n",
    "\n",
    "- Today's introduction to reinforcement learning (RL) is by no means (mathematically) complete\n",
    "- Want to give a high-level overview and some first ideas on the subject to hopefully spark your interest :)\n",
    "- RL is a fascinating field: if you want to learn more, there are some great resources at the end of these slides and on the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8a4fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part I: Introduction to Reinforcement Learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad054d22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Machine learning landscape</h2>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/machine_learning_landscape.png\" alt=\"Machine learning landscape\" style=\"width: 100%;margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.linkedin.com/pulse/business-intelligence-its-relationship-big-data-geekstyle\">GeekStyle</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95045642",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**DeepMind, 2015 & 2017: AlphaGo & AlphaZero**\n",
    "- Famous RL success story: agent learns to play the game of Go and beats world champion Lee Sedol\n",
    "- In case you want to know more: <a href=\"https://www.youtube.com/watch?v=WXuK6gekU1Y\">documentary on YouTube</a>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/alpha_go.png\" alt=\"AlphaGo\" style=\"width: 40%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://www.deepmind.com/research/highlighted-research/alphago\">DeepMind AlphaGo</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71ee86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**OpenAI, 2019: hide-and-seek**\n",
    "- RL agents learning to play hide-and-seek in a multi-agent setting\n",
    "- Recommend to watch the <a href=\"https://openai.com/blog/emergent-tool-use/\">short video</a>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/hide_and_seek.png\" alt=\"Hide and seek\" style=\"width: 40%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://openai.com/blog/emergent-tool-use/\">OpenAI</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1256876",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**DeepMind & EPFL, 2022: tokamak control**\n",
    "- Shaping and maintaining high-temperature plasma within tokamak vessel is challenging\n",
    "- Requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils\n",
    "- Paper describes RL agent that was successfully trained as a magnetic controller\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/tokamak.png\" alt=\"RL for Tokamak\" style=\"width: 80%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://www.nature.com/articles/s41586-021-04301-9\">Paper</a>, <a href=\"https://www.spektrum.de/news/auf-verschlungenen-pfaden/1698480\"> EuroFusion</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e099aa-c388-4373-9fcc-9476a18042b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**DeepMind, 2022: AlphaTensor**\n",
    "\n",
    "- Improve computational efficiency of matrix multiplication\n",
    "- RL agent discovered more efficient algorithms than those developed by humans\n",
    "- Benefits countless fields\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/alpha_tensor.png\" alt=\"AlphaTensor\" style=\"width: 35%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://www.nature.com/articles/s41586-022-05172-4\">Paper</a<p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e31f8a-bb9b-4b94-9e2f-139208b9da47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reinforcement learning examples</h2>\n",
    "\n",
    "**UZH & Intel Labs, 2023: Drone racing**\n",
    "- Training in simulations with mixed-in residual models from real data\n",
    "- RL agent beats human drone racing champions in real environment\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"img/drone_racing.png\" alt=\"DroneRacing\" style=\"width: 50%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"><a href=\"https://www.nature.com/articles/s41586-023-06419-4\">Paper</a></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db138be4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>What is reinforcement learning?</h2>\n",
    "\n",
    "- **Application:** online optimal control, decision-making tasks\n",
    "- **Trial-and-error learning:** for environment in state $s_t$, agent takes action $a_t$ and collects reward $r_t$\n",
    "- **Goal:** learn optimal behavior in an environment\n",
    "    - **Behavior:** a.k.a. policy $\\pi$ - \"what action to pick in a given state?\"\n",
    "    - **Optimality:** maximizing return $G_t = \\sum_{k = 0} \\gamma^k\\,r_{t+k}$  with $\\gamma \\in (0, 1)$\n",
    "- Can be solved in various ways: **many algorithms exist**\n",
    "- We provide **minimal input**: reward function, state definition, action space\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/rl_schematic.png\" alt=\"RL schematically\" style=\"width: 50%;margin-top: 1cm;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a50fcf-d97d-4ab4-ae14-6cc03de794d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>An example: Pacman</h3>\n",
    "\n",
    "- For games it is typically easy to define what the state, actions, and rewards are\n",
    "    - **State:** where am I? Where are ghosts, snacks, cookies?\n",
    "    - **Actions:** up, down, left, right\n",
    "    - **Reward:** food (+), ghosts (-)\n",
    "    - **Return:** how much food am I going to eat over time?\n",
    "    - **Policy:** based on current state of the screen, should I go up, down, left, or right?\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/pacman.png\" alt=\"Pacman example\" style=\"width: 30%; margin-top: 1cm;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408bd4d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Another example: beam trajectory steering</h3>\n",
    "\n",
    "- Maximize integrated beam on target\n",
    "    - **State:** beam position somewhere in the line *(continuous variable)*\n",
    "    - **Action:** increase or decrease dipole kick angle, or strength *(continuous variable)*\n",
    "    - **Reward:** amount of beam on target\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/1d_beam_steering.png\" alt=\"1D beam steering\" style=\"width: 70%; margin-top: 0.5cm;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada026b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Today's lecture</h2>\n",
    "\n",
    "- There are **various RL algorithms** suitable for different types of tasks\n",
    "- Often the choice of algorithm depends on whether we deal with **discrete or continuous state-action spaces**\n",
    "\n",
    "\n",
    "- We will go through:\n",
    "    1. **Discrete states, discrete actions:** Q-learning with a lookup table\n",
    "    2. **Continuous states, discrete actions:** Q-learning with a neural network (deep Q-learning or DQN)\n",
    "    3. **Continuous states, continuous actions:** actor-critic algorithm $\\Rightarrow$ what we typically need to control accelerator systems ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe08629",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Our environment</h2>\n",
    "\n",
    "- A small grid maze\n",
    "- There are fires and a target field\n",
    "- A player, or agent has to navigate through and find the target field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d49da1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env = Maze(height=3, width=5)\n",
    "env.plot(title='Initial state');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c23ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take some actions\n",
    "env.plot(title='Initial state')\n",
    "env.step(action='up')\n",
    "env.plot()\n",
    "env.step(action='right')\n",
    "env.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0fad94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>RL definitions</h3>\n",
    "\n",
    "- **State:** player / agent position (<a style=\"color: #ff0000;\"> <strong>x</strong> </a>) whose coordinates are defined by a tuple $(x, y)$\n",
    "- **Action:** 'up', 'down', 'left', 'right'\n",
    "- **Reward:** every action comes with a reward, depending on the new state we end up in\n",
    "    - Taking a step into an empty field: -1\n",
    "    - Bumping into walls: -5\n",
    "    - Going through fire: -10\n",
    "    - Reaching the goal: +30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c41532",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>RL is about taking the best decisions ... </h2>\n",
    "\n",
    "- Obviously there are **better and worse trajectories to reach the target**. \"Better\" and \"worse\" refer to **how much reward** we can collect along the way.\n",
    "- We will get back to that\n",
    "\n",
    "<p style=\"color: #e6541a;\"> <strong>Exercise 1</strong> </p>\n",
    "Using the reward definitions from the previous slide, try to calculate the cumulative rewards for the trajectories shown below. Can you tell which of the paths are equally good / bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe655035",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/example_trajectories.png\" alt=\"Example trajectories\" style=\"width: 30%;\" />\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20337eb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part II: Reinforcement Learning Formalism</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69f325",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Markov process</h3>\n",
    "\n",
    "- A **memoryless random process** consisting of a **set of states $S$ and state transition probabilities**\n",
    "- The *state* should possess the **Markov property**:\n",
    "    - At every time $t$, the future evolution of the environment depends only on the information contained in the current state $s_t$, but not on the history of past states $s_{t-1}, s_{t-2}, ...$ *(memorylessness)*\n",
    "    - In other words: $P(s_{t+1} | s_t) = P(s_{t+1} | s_{t}, s_{t-1}, ..., s_0)$\n",
    "- **Examples:**\n",
    "    - Chess: the positions of the figures on the board fully define the state. There are many ways to reach the specific state, but it does not matter for making the next move nor for the future evolution of the game.\n",
    "    - Flight of a cannonball: state given by its current position and velocity vector provides enough information to predict the future.\n",
    "      *(non-Markov: if state only contains the current position, but not the velocity)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f9dd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/markov_chain.png\" alt=\"Markov chain\" style=\"width: 45%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf\">D. Silver - Lecture on RL</a></p>\n",
    "    \n",
    "</center>\n",
    "\n",
    "$S = \\{\\text{Class 1, Class 2, Class 3, Facebook, Pub, Pass, Sleep}\\}$  \n",
    "*Note that \"Sleep\" is also called a **terminal state**, because once in it we will never leave it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3ce97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Markov reward process</h3>\n",
    "\n",
    "- A Markov process that has in addition a **reward function** and a **discount factor** $\\gamma \\in [0, 1]$\n",
    "- **Return** $G_t$: sum of discounted future rewards\n",
    "\n",
    "$$G_t = \\sum_k \\gamma^k \\, r_{t+k}$$\n",
    "\n",
    "- $\\gamma$ controls the relative importance of immediate vs future rewards\n",
    "    - $\\gamma \\rightarrow 0$: we only care about immediate rewards\n",
    "    - $\\gamma \\rightarrow 1$: we care about rewards far in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb1b4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"img/markov_reward_process.png\" alt=\"Markov reward process\" style=\"width: 40%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf\">D. Silver - Lecture on RL</a></p>\n",
    "    \n",
    "</center>\n",
    "\n",
    "- **Example:** C1 $\\rightarrow$ C2 $\\rightarrow$ C3 $\\rightarrow$ Pass $\\rightarrow$ Sleep\n",
    "<br>$\\Rightarrow$ Return: $G_0 = (-2) + 0.5 * (-2) + 0.5^2 (-2) + (+10) * 0.5^3 = -2.25$ *(with $\\gamma = 0.5$)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff09b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Markov decision process (MDP)</h3>\n",
    "\n",
    "- Extend Markov reward process by adding **decision making**: set of possible actions $A$ (= action space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71446421",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"img/markov_decision_process.png\" alt=\"Markov decision process\" style=\"width: 40%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf\">D. Silver - Lecture on RL</a></p>\n",
    "    \n",
    "</center>\n",
    "\n",
    "\n",
    "<br> *N.B.: stochastic state transitions are still allowed (if we decide to go to the Pub, anything can happen).*  \n",
    "*Today we will work with fully deterministic MDPs only.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d13fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Episodic MDP**\n",
    "    - Each episode ends in a terminal state\n",
    "    - Return $G_t$ is the sum of discounted rewards collected from time $t$ till end of episode\n",
    "    - Episodes are independent\n",
    "\n",
    "\n",
    "- **Continuous MDP**\n",
    "    - Continues indefinitely: has no terminal states\n",
    "    - Very important that discount factor $\\gamma < 1$ to avoid infinite returns\n",
    "    - Also known as infinite horizon MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b3cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Policy $\\pi$</h3>\n",
    "\n",
    "- The policy defines the decision making or **behavior of the agent**\n",
    "- It is a **probability distribution over the state-action space**. You can also think of it as a mapping that assigns to each state-action pair $(s, a)$ a probability\n",
    "\n",
    "$$\\pi: S \\times A \\rightarrow [0, 1]$$\n",
    "\n",
    "\n",
    "- $S$ and $A$ are the state and action spaces, respectively\n",
    "    - For our maze: $S = \\{[0, 0], [0, 1], ..., [\\text{width}-1, \\text{height}-1]\\}$ and $A = \\{\\text{'up', 'down', 'left', 'right'}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b097255",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 2</strong> </p>\n",
    "\n",
    "Let's get back to the maze! For now we do not care about optimal decisions. Instead, try to **implement a random policy**, i.e. every action $a \\in \\{\\text{'up', 'down', 'left', 'right'}\\}$ is picked with equal probability no matter what state the agent is in.\n",
    "\n",
    "**a)** Initialize a Maze with `height=3, width=2` and complete the `all_actions` list.\n",
    "\n",
    "**b)** Look at every step of the output: is the movement of the agent (<a style=\"color: #ff0000;\"> <strong>x</strong> </a>) and the rewards obtained consistent with your expectations?\n",
    "\n",
    "**c)** Change the random number seed, rerun and observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8329c86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123457)\n",
    "\n",
    "env = Maze( # FILL HERE)\n",
    "env.plot(title='Initial state')\n",
    "\n",
    "all_actions = [ # ... FILL HERE]\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.choice(all_actions)\n",
    "    state, action, reward, new_state, done = env.step(action)\n",
    "    env.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ebb0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>RL objective</h2>\n",
    "\n",
    "- **Find optimal behavior in a given environment:** in every state we want the agent to take the best action\n",
    "- This is also known as the **optimal policy** $\\pi^*$\n",
    "- Formally, $\\pi^*$ **maximizes the return** $G_t = \\sum_k \\gamma^k \\, r_{t+k}$, i.e. the cumulative sum of discounted future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6c267",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **For our maze, RL will solve ...**\n",
    "    - For any given field that we are currently on (= state), what is the action that maximizes the sum of rewards collected over time?\n",
    "    - Or: from where I stand - how can I reach the target field with the **least steps and not going through fires** (if possible) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb7619",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>RL taxonomy</h2>\n",
    "\n",
    "- There are many **different algorithms** for finding the optimal policy $\\pi^*$\n",
    "- They all have their pros and cons\n",
    "    - Often the **sample-efficiency** is crucial\n",
    "    - It tells us **how many interactions with the environment** (= how many data samples) we need to solve the RL problem\n",
    "    - E.g. for **accelerator systems**: we want to train the agent with **as little beam time as possible** as it is very expensive\n",
    "    \n",
    "    \n",
    "- **Today:** we are going to look at **Q-learning**. It is one of the **core ideas** of many RL algorithms, such as\n",
    "    - Deep Q-learning (DQN)\n",
    "    - Actor-critic methods (DDPG, TD3, SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc158352",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/rl_taxonomy.png\" alt=\"The RL algorithm zoo\" style=\"width: 55%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html\">Open AI - Spinning Up</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4873170",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/sample_efficiency.png\" alt=\"Sample efficiency\" style=\"width: 70%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image adapted from <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">S. Levine, \"Deep Reinforcement Learning\" (lecture)</a></p>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acfb087",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"color: #b51f2a\">Intermediate summary</h2>\n",
    "\n",
    "- The goal of RL is to **make optimal decisions** (*take actions*) in an environment based on some observables (*state*)\n",
    "- **Example environments:** game, control system (e.g. fusion reactor, tuning accelerator parameters), trading, ...\n",
    "- The **quality of a decision** made is quantified by a **reward**\n",
    "- Through **trial-and-error** the RL agent collects rewards and can eventually learn the **best behavior** (*optimal policy* $\\pi^*$)\n",
    "- Formally this is described as a **Markov decision process (MDP)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845e215",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part III: Q-learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b523a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Q-learning</h2>\n",
    "\n",
    "- Employs a **state-action value function**\n",
    "$$Q: S \\times A \\rightarrow \\mathbb{R}$$\n",
    "to solve the RL problem\n",
    "\n",
    "\n",
    "- The Q-value $Q(s, a)$ characterizes the **\"quality\" of the state-action pair** $(s, a)$\n",
    "    - Quality is measured as the **expected return** acting according to a certain policy\n",
    "    $$Q(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a]$$\n",
    "    - Reminder: $G_t = \\sum_k \\gamma^k \\, r_{t+k}$\n",
    "    - Q answers: *“In a given state, what is the best action to take to maximize return?”*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a4550",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Temporal difference (TD) learning** or **learning the Q-function**\n",
    "    - We can write the Q-value of $(s_t, a_t)$ as a sum of immediate reward $r$ plus discounted Q-value of the next state-action pair $(s_{t+1}, a_{t+1})$\n",
    "    - We are acting greedily, hence the $\\text{max}$ operation*\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/td_tree.png\" alt=\"Q-learning backup diagram\" style=\"width: 25%; margin-top: 1cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">\n",
    "    \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee295e62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Temporal difference (TD) rule**\n",
    "    - Q-values are **initially unknown / random**\n",
    "    - Learn **iteratively** following TD update rule **using collected interactions** with environment\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/bellman.png\" alt=\"Q-learning backup diagram\" style=\"width: 50%; margin-top: 1cm;\" />\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "- Use **trial-and-error experiences** collected by the agent $(s_t, a_t, r_t, s_{t+1})$: *state, action, reward, next state*.\n",
    "- This is the **core idea of Q-learning** and is a result of one of the Bellman equations\n",
    "- **Bootstrapping:** update Q-value using target based on estimate $\\rightarrow$ a moving target”: training could be unstable, hence double Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974301a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Obtaining the optimal policy</h2>\n",
    "\n",
    "- Once Q-values have converged, it is easy to read off the optimal policy $\\pi^*$\n",
    "\n",
    "$$\n",
    "\\pi^*(s, a) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1 & \\mbox{if } a = \\text{argmax}_{a'} \\, Q(s, a') \\\\\n",
    "        0 & \\mbox{otherwise.}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "- This is also known as the **greedy policy**: it acts greedily in terms of expected return by assigning probability $1$ to the action that maximizes the Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a125d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>How to implement Q-learning?</h2>\n",
    "\n",
    "- We need a way to **track and update the Q-value** for each state-action pair\n",
    "    - Traditional Q-learning: **Q-table**\n",
    "    - Deep Q-learning: **neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4378b1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/qlearn_dqn.png\" alt=\"Q-learning vs DQN\" style=\"width: 45%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.assemblyai.com/blog/reinforcement-learning-with-deep-q-learning-explained/\">AssemblyAI</a></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62414694",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Some challenges</h2>\n",
    "\n",
    "- Reward engineering\n",
    "- Definition of the state, which is sometimes only partially observable\n",
    "- Hyperparameter tuning, making training stable\n",
    "- Exploration vs exploitation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bbec3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Exploration-exploitation trade-off</h3>\n",
    "\n",
    "- To learn the best policy in the most efficient manner, we need a **trade-off between exploration and exploitation**.\n",
    "- We have to **ensure that the agent keeps exploring** new actions during training and does not just always follow the path that provides the highest expected return\n",
    "- After all, Q-values might **not have converged yet**, there may still be better solutions than currently known\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/exploration_exploitation_tradeoff.jpg\" alt=\"Exploration-exploitation tradeoff\" style=\"width: 40%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec15_6up.pdf\">Berkeley AI course</a></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5d480",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Q-learning with lookup table</h2>\n",
    "\n",
    "- The first Q-learning method we consider is using a **lookup table** to keep track of the Q-values during training\n",
    "- First, we **initialize** all Q-values **to 0**, then **update** the values according to the **TD rule**\n",
    "- *N.B.: this method works only for discrete sets of states and actions*\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/qtable.png\" alt=\"Q-table learning\" style=\"width: 80%;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f873d0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Initialize small maze environment\n",
    "env = Maze(width=2, height=2, fire_positions=[[1, 0]])\n",
    "_ = env.plot(add_player_position=False)\n",
    "\n",
    "# Initialize Q-learner with Q-table\n",
    "qtable_learner = QLearner(env, q_function='table')\n",
    "\n",
    "print('Initial Q-table')\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "print_qtable(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066165b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qtable_learner.train(200)\n",
    "\n",
    "print('Q-table after 200 episodes')\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "print_qtable(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4891a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qtable_learner.train(300)\n",
    "\n",
    "print('Q-table after 500 episodes')\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "print_qtable(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4be4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qtable_learner.plot_training_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba15a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 3</strong> </p>\n",
    "\n",
    "**a)** Based on the evolution of the Q-values on the previous slide - would you consider the training to be complete after 500 episodes?\n",
    "\n",
    "**b)** Play with the number of episodes in the cell below until you find convergence.\n",
    "\n",
    "**c)** Observe that some of the Q-table values converge earlier than others during training. Why could that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46d276",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "env = Maze(width=2, height=2, fire_positions=[[1, 0]])\n",
    "\n",
    "qtable_learner = QLearner(env, q_function='table')\n",
    "qtable_learner.train( # FILL HERE)\n",
    "qtable_learner.plot_training_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a950c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 4</strong> </p>\n",
    "\n",
    "**a)** Initialize a bigger maze `width=4`, `height=3`, with `fire_positions=[[2, 1], [2, 2]]` and use `q_function='table'` in the `QLearner` class. Then train it for `5000` episodes. \n",
    "\n",
    "**b)** Once the training is finished, plot the Q-values (you can just execute the cell, it is already complete).\n",
    "<br>*Next to each little arrow there is a number that denotes the Q-value of the corresponding action on that field. The red arrow indicates the action with the highest Q-value.*\n",
    "\n",
    "**c)** Finally, also plot the (greedy) policy by executing the third cell. Compare it to the Q-value plot to verify that we indeed always pick the action with the highest Q-value. Are there fields where two actions would be equally good (which ones)? Can you confirm that by looking at the Q-values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358c689",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 4 a)\n",
    "np.random.seed(123456)\n",
    "\n",
    "env = Maze(width= # FILL HERE,\n",
    "           height= # FILL HERE,\n",
    "           fire_positions= # FILL HERE)\n",
    "\n",
    "qtable_learner = QLearner(env, q_function= # FILL HERE)\n",
    "qtable_learner.train( # FILL HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974158c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the training does not work for you for some reason\n",
    "# you can reload the qtable from a trained agent from file.\n",
    "# Don't forget to initialize the env as suggested in the\n",
    "# exercise ...\n",
    "# Note that the q evolution history of training is not saved\n",
    "# and will hence not be displayed when reloading from file.\n",
    "\n",
    "# qtable_learner.q_func.load_q_table('saved_agents/qtable_ex4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dc93f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 4 b)\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "ax = env.plot(add_player_position=False, title=False)\n",
    "plot_q_table(q_table, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dc5cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 4 c)\n",
    "policy = qtable_learner.q_func.get_greedy_policy()\n",
    "ax = env.plot(add_player_position=False, title=False)\n",
    "plot_greedy_policy(policy, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0861a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong>Exercise 5</strong> (optional)</p>\n",
    "\n",
    "**a)** Using the same maze as above, reduce the punishment of going through fire by setting a `fire_reward=-2` (instead of -10) in the environment definition.\n",
    "\n",
    "**b)** Retrain the agent. How does the policy change compared to Ex. 4? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff6304",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "\n",
    "# Env definition\n",
    "env = Maze(width=4, height=3, fire_positions=[[2, 1], [2, 2]], fire_reward= # FILL HERE)\n",
    "qtable_learner = QLearner(env, q_function='table')\n",
    "qtable_learner.train(500)\n",
    "\n",
    "# If you have issues with the training, please comment out the\n",
    "# line qtable_learner.train(5000) above and reload instead the\n",
    "# qtable by uncommenting the following line.\n",
    "\n",
    "# qtable_learner.q_func.load_q_table('saved_agents/qtable_ex5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981af54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show Q-values\n",
    "q_table = qtable_learner.q_func.get_q_table()\n",
    "ax = env.plot(add_player_position=False, title=False)\n",
    "plot_q_table(q_table, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980a26e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show policy\n",
    "policy = qtable_learner.q_func.get_greedy_policy()\n",
    "ax = env.plot(add_player_position=False, title=False)\n",
    "plot_greedy_policy(policy, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af22654",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Deep Q-learning (DQN)</h2>\n",
    "\n",
    "- **Main idea:** replace the Q-table by a simple, feed-forward neural network (Q-net)\n",
    "- Developed by DeepMind in 2013 to play Atari games *(<a href=\"https://arxiv.org/abs/1312.5602\">DQN paper</a>)*\n",
    "- A neural network (NN) is a **universal function approximator**, i.e. a fit model that can approximate any function (in theory)\n",
    "- The Q-net is a **mapping** from **state to Q-values** of all possible actions\n",
    "- Its parameters a.k.a. weights are adjusted according to the **TD rule**, like the Q-table\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/qnet_neuron.png\" alt=\"Q-net\" style=\"width: 90%; margin-top: 0.5 cm;\" />\n",
    "<p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\"></p>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3bb83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong> Exercise 6</strong> </p>\n",
    "\n",
    "**a)** Repeat the same steps as in Ex. 4 for the Q-table learner, but this time using `q_function='net'` as an argument in the `QLearner` class. Train it for `1500` episodes. This will take a couple of minutes.\n",
    "\n",
    "**b)** Compare the Q-values and policy to the one obtained with Q-table learning. Do you see differences? Why could that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487f614",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "env = Maze(width=4, height=3, fire_positions=[[2, 1], [2, 2]])\n",
    "qnet_learner = QLearner(env, q_function= # FILL HERE)\n",
    "qnet_learner.train( # FILL HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, if you face any issues with model training, please use\n",
    "# the saved q-net weights of a trained agent by uncommenting\n",
    "# the following. You can comment the line for training in the\n",
    "# previous cell.\n",
    "\n",
    "# qnet_learner.q_func.load_model('saved_agents/qnet_ex6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6d288",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "q_table = qnet_learner.q_func.get_q_table()\n",
    "ax = env.plot(add_player_position=False, title=False)\n",
    "plot_q_table(q_table, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbf9f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "policy = qnet_learner.q_func.get_greedy_policy()\n",
    "ax = env.plot(add_player_position=False, title=False)\n",
    "plot_greedy_policy(policy, env.target_position, env.fire_positions, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8e384",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Q-table vs DQN: pros, cons, and limitations</h2>\n",
    "\n",
    "- **Q-table**\n",
    "    - <p style=\"color: #228B22;\">Easy to understand and validate</p>\n",
    "    - <p style=\"color: #A91E1E;\">Discrete $S$, $A$ spaces only</p>\n",
    "    - <p style=\"color: #A91E1E;\">Relatively small $S$, $A$ spaces only</p>\n",
    "- **DQN**\n",
    "    - <p style=\"color: #228B22;\">Big and continuous $S$ possible</p>\n",
    "    - <p style=\"color: #228B22;\">No need to visit all states during training, because NNs are great interpolators</p>\n",
    "    - <p style=\"color: #A91E1E;\">Discrete and relatively small $A$</p>\n",
    "    - <p style=\"color: #A91E1E;\">Training may be unstable and harder to verify if we have reached convergence</p>\n",
    "- Many real-world problems require **continuous $S$ *and* continuous $A$** $\\,\\,\\Rightarrow\\,\\,$ **actor-critic methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041e506",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center; width:100%;\">\n",
    "    <h2>Part IV: Actor-critic Methods</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08fbc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Actor-critic scheme</h2>\n",
    "\n",
    "- **Two NNs**\n",
    "    \n",
    "- **Actor**\n",
    "    - Represents the policy $\\pi$ and is a mapping $\\pi: S \\rightarrow A$\n",
    "    - For each **continuous state**, it proposes a **continuous action**\n",
    "    - Learns from the critic\n",
    "    \n",
    "    \n",
    "- **Critic**\n",
    "    - Predicts Q-values and is a mapping $Q: S\\times A \\rightarrow \\mathbb{R}$\n",
    "    - Evaluates quality of $(s, a)$ pair proposed by actor\n",
    "    - Feeds back to the actor network: policy gradient rule\n",
    "\n",
    "\n",
    "*N.B.: networks are trained simultaneously*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5c179",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"img/actor_critic.png\" alt=\"Actor-critic schematic\" style=\"width: 70%;\" />\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463cafbd",
   "metadata": {},
   "source": [
    "- **Critic parameters** $\\theta$ are updated according to the **TD rule**, just like in Q-learning\n",
    "- **Actor parameters** $\\chi$ are updated via **policy gradient**: for a given state $s$, how does the actor have to adjust its parameters to propose an action $a$ such that $Q(s,a)$ becomes larger?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683f6ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Application in accelerator physics</h2>\n",
    "\n",
    "- We are going to consider a **trajectory steering problem** from CERN's **AWAKE**\n",
    "- <a href=\"https://www.nature.com/articles/s41586-018-0485-4\">Advanced Proton Driven Plasma Wakefield Acceleration Experiment</a>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/awake.png\" alt=\"AWAKE\" style=\"width: 85%; margin-top: 1cm\"/> <p style=\"clear: both; font-size: 10pt; text-align: right; float: right;\">image by <a href=\"https://www.nature.com/articles/s41586-018-0485-4\">AWAKE Collaboration</a></p>\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d24a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>AWAKE electron beam line</h3>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/awake_beamline.png\" alt=\"AWAKE beamline\" style=\"width: 50%; margin-top: 1cm;\"/>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd924e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>RL task definitions</h3>\n",
    "\n",
    "- **Goal:** given measured beam positions *(= continuous state)*, find best dipole corrector settings *(= continuous actions)* to keep beam close to the center of vacuum pipe\n",
    "- **State:** 10-d array of beam positions measured along the line\n",
    "- **Action:** 10-d array of dipole corrector strengths along the line\n",
    "- **Reward:** negative rms of beam offsets wrt. center\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"img/trajectory_task.png\" alt=\"Electron beam line steering task\" style=\"width: 40%; margin-top: 0.5cm;\"/>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca5d30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"color: #e6541a;\"> <strong> Exercise 7</strong> </p>\n",
    "\n",
    "**Let's try to train an actor-critic agent on the AWAKE environment!** We are using the **DDPG** *(<a href=\"https://spinningup.openai.com/en/latest/algorithms/ddpg.html\">Deep Deterministic Policy Gradient</a>)* algorithm. It is one of the most basic actor-critic algorithms and hence also not the most stable one. Some improvements have been implemented in TD3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac01940",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**7 a)** Run the following cell to initialize the AWAKE simulation environment `env` and a DDPG instance `agent`. Then reset the environment to misteer the beam, and plot the trajectory. The plot shows the beam position at the 10 BPMs installed along the electron beam line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f28a46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 a)\n",
    "tf.keras.utils.set_random_seed(12345)\n",
    "\n",
    "env = e_trajectory()\n",
    "agent = ClassicalDDPG(state_space=env.observation_space, action_space=env.action_space)\n",
    "\n",
    "env.reset(init_outside_threshold=True)\n",
    "env.plot_trajectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa566a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**7 b)** Run the next cell to make a correction to the beam position. Run the cell multiple times and check how the trajectories before and after correction compare. Do you think the RL agent is doing a good job? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76ff30",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 b)\n",
    "run_correction(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a597be4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**7 c)** Run the next cell to train the RL agent. Can you interpret the output plots showing evolution of agent training? Is the length of training appropriate or should we train with fewer / more steps?\n",
    "\n",
    "*Hints: the output figure shows two axes. The top graph displays the length of each episode over the entire training period (an episode is terminated either when the objective is reached or whenever the agent cannot solve the task after 30 steps). The bottom plot shows the rewards (negative trajectory rms) at the beginning and at the end of an episode. A high negative reward means that the trajectory is badly steered. A reward close to zero on the other hand corresponds to a well-corrected beam trajectory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89b0a9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 c)\n",
    "training_log = trainer(env=env, agent=agent, n_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d61cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_training_log(env, agent, training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e354fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In case you face any issues with the training of the agent,\n",
    "# avoid executing the previous cell and use instead the \n",
    "# following line to load pre-trained weights.\n",
    "\n",
    "# agent.load_actor_critic_weights('saved_agents/ddpg_ex7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4277c40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**7 d)** Check a few trajectories before and after correction now using the trained agent (run the cell multiple times). How does the agent perform now?\n",
    "\n",
    "*Note that we are using the DDPG algorithm here which is one of the most basic actor-critic RL algorithms. This is also the reason why the trajectory correction will not always be perfect. There are much improved versions that would train in a shorter time and with better performance (e.g. <a href=\"https://spinningup.openai.com/en/latest/algorithms/td3.html\">Twin Delayed DDPG</a> aka TD3).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5f8c2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 7 d)\n",
    "run_correction(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e53a18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Summary</h2>\n",
    "\n",
    "- Reinforcement learning (RL) is concerned with **solving decision-making problems** and optimizing for **best behavior** in an environment\n",
    "- **Different algorithms** exist with Q-learning being one of the fundamental ones\n",
    "- **Q-learning uses a state-action value function $Q(s,a)$** that estimates the expected return\n",
    "- $Q(s, a)$ is iteratively learned following the **temporal difference rule**\n",
    "- Once converged we can **read off the optimal policy by acting greedily** with respect to Q\n",
    "- **Q-learning**\n",
    "    - Lookup table: only works for **discrete state-action spaces**\n",
    "    - Q-net (neural network): can deal with **continuous states**\n",
    "- **Actor-critic** methods are built on top of Q-learning and use **two networks**:\n",
    "    - One for the policy (actor) and one to estimate the Q-values (critic)\n",
    "    - They can solve tasks with **continuous states *and* actions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde19edd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Comprehension questions I</h2>\n",
    "\n",
    "- What do Q-values represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c442ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why does the Q-table only work for discrete state space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3dbc44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why does Q-learning only work for discrete action spaces, both for the Q-table and the Q-net?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dce7fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How can you obtain the optimal policy once you know the Q-values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78fa55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Comprehension questions II</h2>\n",
    "\n",
    "- Do you have to increase or decrease the discount factor $\\gamma$ to put more emphasis on future rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78317a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the Markov property? Is it fulfilled for the maze environment? Why, or why not? (*hint: think about whether it matters for the future evolution how you ended up on the current field of the maze*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0492ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For the temporal difference learning update, how many different states are involved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e56e81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Literature</h2>\n",
    "\n",
    "- R.S. Sutton and A.G. Barto, <a href=\"http://incompleteideas.net/book/RLbook2020.pdf\">\"Reinforcement learning - an introduction\"</a>, Book, 2nd edition, 2020.\n",
    "- S. Levine, <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">Deep Reinforcement Learning</a>, Lecture, UC Berkeley, 2022.\n",
    "- D. Silver, <a href=\"https://www.davidsilver.uk/teaching/\">Reinforcement learning</a>, Lecture, University College London (UCL), 2015.\n",
    "\n",
    "\n",
    "<h2>Python RL libraries</h2>\n",
    "\n",
    "- Stable baselines 3: <a href=\"https://github.com/DLR-RM/stable-baselines3\">github</a>, <a href=\"https://stable-baselines3.readthedocs.io/en/master/\">docs</a>\n",
    "- Gymnasium: <a href=\"https://github.com/Farama-Foundation/Gymnasium\">github</a>, <a href=\"https://gymnasium.farama.org/content/basic_usage/\">docs</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<p>Fachbereich Elektrotechnik und Informationstechnik (etit)   |   Institut für Teilchenbeschleunigung und Elektromagnetische Felder (TEMF)   |   Dr. Michael Schenk</p>",
   "header": "<img src='https://upload.wikimedia.org/wikipedia/de/thumb/2/24/TU_Darmstadt_Logo.svg/640px-TU_Darmstadt_Logo.svg.png' />",
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
